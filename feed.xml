<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://doronpor.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://doronpor.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-14T17:29:09+00:00</updated><id>https://doronpor.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Towards Universal Navigation Foundation Models</title><link href="https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/" rel="alternate" type="text/html" title="Towards Universal Navigation Foundation Models"/><published>2025-10-13T08:40:00+00:00</published><updated>2025-10-13T08:40:00+00:00</updated><id>https://doronpor.github.io/blog/2025/2025-10-13-NavFoM</id><content type="html" xml:base="https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/"><![CDATA[<p>This post introduces the <a href="https://arxiv.org/abs/2509.12129">Embodied Navigation Foundation Model</a> paper [Liu et al., 2025] — a cross-embodiment and cross-task navigation framework termed Navigation Foundation Model (NavFoM).</p> <h2 id="navigation-meets-foundation-model">Navigation meets Foundation Model</h2> <p>Classical navigation models are often limited to a <strong>single robot or task</strong> - a ground robot model can’t generalize to a drone or a language-guided setup. Meanwhile, <strong>foundation models</strong> in vision and language (like CLIP and GPT-4V) have shown that large-scale multi-modal pretraining can yield broad generalization. In parallel, recent advancements in <strong>Vision-Language-Action (VLA)</strong> models have shown promise for <strong>end-to-end autonomous driving</strong>, where large language models (LLMs) are integrated with perception and planning modules to perform end-to-end reasoning and planning.</p> <p>Inspired by this, <a href="https://arxiv.org/abs/2509.12129">Embodied Navigation Foundation Model</a> introduced a cross-embodiment and cross-task navigation framework termed Navigation Foundation Model (<strong>NavFoM</strong>). NavFoM is trained on over eight million navigation samples spanning <strong>quadrupeds, drones, wheeled robots, and vehicles</strong>. The dataset covers a diverse set of tasks — including <strong>vision-and-language navigation</strong>, <strong>object searching</strong>, <strong>target tracking</strong>, and <strong>autonomous driving</strong> - allowing the model to generalize on cross-task and cross-embodiment.</p> <p>The experiments show that the <strong>NavFoM</strong> - makes significant progress toward generalist navigation. Without any task-specific fine-tuning, the model achieves <strong>state-of-the-art or competitive performance</strong> across multiple public task benchmarks and robot embodiments.</p> <figure align="center"> <img src="/assets/img/posts/ENFM_high_level_arch.jpg" width="90%"/> <figcaption><b>Figure 1.</b> Architecture illustration. Adapted from <a href="https://arxiv.org/abs/2509.12129">Liu et al., 2025</a>.</figcaption> </figure> <h3 id="key-results">Key results</h3> <ul> <li> <p><strong>VLN-CE RxR (Vision-Language Navigation)</strong></p> <ul> <li>Multi-camera: <strong>64.4% SR</strong> ↑ <em>(from 56.3%)</em></li> <li>Single-camera: <strong>57.4% SR</strong> ↑ <em>(from 51.8%)</em></li> </ul> </li> <li> <p><strong>HM3D-OVON (Object-Goal Navigation)</strong></p> <ul> <li>Zero-shot: <strong>45.2% SR</strong> ↑ (<em>from previous fine-tuned SOTA 43.%</em>)</li> </ul> </li> </ul> <hr/> <h2 id="how-the-model-works">How the Model Works</h2> <p><strong>NavFoM</strong> employs a unified architecture that processes multi-modal navigation inputs from varying camera configurations and navigation horizons. The authors introduces two key innovations: <strong>Temporal-Viewpoint Indicator (TVI)</strong> tokens, which encode camera viewpoint and temporal context, and <strong>Budget-Aware Temporal Sampling (BATS)</strong>, which dynamically selects history tokens using a forgetting curve within a fixed token budget.</p> <h3 id="architecture-overview">Architecture Overview</h3> <p>The model consists of four major components:</p> <ol> <li> <p><strong>Vision Encoder</strong> - processes egocentric RGB from different camera configurations (single or multi-camera) based on <a href="https://arxiv.org/abs/2304.07193">DINOv2</a> and <a href="https://arxiv.org/abs/2303.15343">SigLip</a>.</p> </li> <li> <p><strong>Cross-Modal projector</strong> - Vision encoder is followed by a cross-modal project to align the image embeddings to the text embeddings.</p> </li> <li> <p><strong>Large Language Model</strong> - Except text instructions, projected image embeddings and temporal viewpoint indicator tokens and output tokens to to the Action head. The model uses a pretrained large language model (LLM) backbone <a href="https://arxiv.org/abs/2407.10671">Qwen2-7B</a>.</p> </li> <li> <p><strong>Trajectory prediction head</strong> - outputs continuous trajectory, normalized to [-1,1] and scaled according to the output task. Trajectory head is Implemented as a three-layer MLP.</p> </li> </ol> <figure align="center"> <img src="/assets/img/posts/navform_pipeline.jpg" width="90%"/> <figcaption><b>Figure 2.</b> NavForm pipeline. Adapted from <a href="https://arxiv.org/abs/2509.12129" target="_blank">Liu et al., 2025</a>.</figcaption> </figure> <h3 id="encoding-temporal-information-and-camera-setup">Encoding temporal information and camera setup</h3> <blockquote> <p><strong>Challenge</strong>: model must understand when and where (which view) each observation was captured.</p> </blockquote> <p>A key challenge in multi-view navigation is that visual tokens alone don’t tell the model which camera or timestep they came from. To address this, NavFoM introduces <strong>Temporal-Viewpoint Indicator (TVI)</strong> tokens — lightweight learnable embeddings that tag each frame with both its viewpoint angle and its temporal position in the sequence.</p> <p>Each TVI token combines three components:</p> <ul> <li> <p><strong>Angle embedding</strong>: captures the camera’s azimuth in a circular way (0° ≡ 360°) so nearby viewpoints are geometrically close in embedding space.</p> </li> <li> <p><strong>Time embedding</strong>: marks the temporal order of frames, allowing reasoning over navigation history.</p> </li> <li> <p><strong>Base embedding</strong>: provides a consistent token structure that the model can adapt across different tasks (navigation, video QA, image QA).</p> </li> </ul> \[\mathbf{E}_{\text{TVIT}} = \begin{cases} \mathbf{E}_{\text{Base}} + \mathcal{P}_{\text{time}}\big(\text{TimePE}(t)\big) + \mathcal{P}_{\text{angle}}\big(\text{AnglePE}(\phi)\big), &amp; \text{if Navigation} \\ \mathbf{E}_{\text{Base}} + \mathcal{P}_{\text{time}}\big(\text{TimePE}(t)\big), &amp; \text{if Video QA} \\ \mathbf{E}_{\text{Base}}, &amp; \text{if Image QA} \end{cases}\] <p>This flexible design lets NavFoM process arbitrary camera configurations from single-view robots to panoramic car rigs, while maintaining clear separation between views and timestep.</p> <div class="alert alert-info" role="alert"> 💡 This method only incorporate a single DOF to encode the view of the camera. This may be restrictive in some situations like UAV with downward facing camera (see evaluation section) </div> <h3 id="adapting-to-token-budget">Adapting to token budget</h3> <blockquote> <p><strong>Challenge:</strong> Balancing long-term reasoning with limited compute and speed in real world</p> </blockquote> <p>Long navigation sequences can produce thousands of visual tokens — far more than a transformer can handle efficiently for real time real world scenarios. To maintain temporal reasoning without exceeding memory limits, <strong>NavFoM</strong> introduces <strong>Budget-Aware Temporal Sampling (BATS)</strong>.</p> <p>Rather than treating every timestep equally, <strong>BATS applies a forgetting curve</strong> that assigns higher priority to recent frames and lower priority to distant ones. From all past frames {1, 2, …, T}, the model selects a subset B:</p> \[|\mathcal{S}(T)| = B, \quad \mathcal{S}(T) \subseteq \{1, 2, \ldots, T\}\] <p>The forgetting curve is modeled as a simple exponential-like decay:</p> \[P(t, T) \propto e^{-\lambda (T - t)}\] <p>This ensures the model maintains a <strong>constant token budget</strong> while focusing attention on the most recent and relevant observations. The result is a <strong>balanced temporal representation</strong> that achieves near-equivalent performance to full-sequence training while significantly reducing inference time.</p> <p>Explore the sampling function:</p> <div class="alert alert-info" role="alert"> 💡 Adjusting to high budget with low view count will sample each frame </div> \[P(t) = (1 - \varepsilon)e^{k(t - T)/T} + \varepsilon, \quad E_{\text{frames}} = T \left[ (1 - \varepsilon)\frac{1 - e^{-k}}{k} + \varepsilon \right]\] <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <div id="bats-real-plot" style="width:100%;height:420px;"></div> <script>
function E_frames(k, eps, T) {
  // Stable version for small k
  if (Math.abs(k) < 1e-6) return T;
  return T * ((1 - eps) * (1 - Math.exp(-k)) / k + eps);
}

function solveK(target_E, eps, T) {
  // adaptive bounds
  let low = 1e-6, high = 50;
  const target = target_E;
  const fLow = E_frames(low, eps, T);
  const fHigh = E_frames(high, eps, T);

  // if outside range, clamp
  if (target >= fLow) return low;
  if (target <= fHigh) return high;

  for (let i = 0; i < 80; i++) {
    const mid = 0.5 * (low + high);
    const fMid = E_frames(mid, eps, T);
    if (fMid > target) low = mid;
    else high = mid;
  }
  return 0.5 * (low + high);
}

function generateData(Btoken=1600, N=4, eps=0.1, T=125) {
  let E_target = (Btoken / N - 65) / 5;
  if (!isFinite(E_target) || E_target < 1) E_target = 1;
  if (E_target > T - 1) E_target = T - 1;

  const k = solveK(E_target, eps, T);
  const t = Array.from({length: T}, (_, i) => i + 1);
  const P = t.map(x => (1 - eps) * Math.exp(k * (x - T) / T) + eps);
  const maxP = Math.max(...P);
  const normP = P.map(p => p / maxP);
  return {t, P: normP, k, E_target};
}

function drawPlot(Btoken, N) {
  const eps = 0.1, T = 125;
  const {t, P, k, E_target} = generateData(Btoken, N, eps, T);
  const trace = {
    x: t,
    y: P,
    type: 'scatter',
    mode: 'lines',
    line: {color: '#00bcd4', width: 4},
    fill: 'tozeroy',
    fillcolor: 'rgba(0,188,212,0.25)',
  };
  const layout = {
    margin: {t: 50, l: 60, r: 20, b: 50},
    title: {text: `BATS — B tokens=${Btoken}, N=${N}, k≈${k.toFixed(2)}, E frames ≈${E_target.toFixed(1)}`, font: {size: 14}},
    xaxis: {title: 'timestep (t)', gridcolor: 'rgba(255,255,255,0.1)'},
    yaxis: {title: 'P(t)', gridcolor: 'rgba(255,255,255,0.1)'},
    plot_bgcolor: 'rgba(0,0,0,0)',
    paper_bgcolor: 'rgba(0,0,0,0)',
    font: {color: '#eee', size: 14}
  };
  Plotly.newPlot('bats-real-plot', [trace], layout, {displayModeBar: false});
}

let Btoken = 1600, N = 4;
drawPlot(Btoken, N);

const controls = document.createElement('div');
controls.innerHTML = `
  <label>B tokens: <input id="B-slider" type="range" min="640" max="2048" step="64" value="1600"></label>
  <span id="B-val">1600</span><br>
  <label>N views: <input id="N-slider" type="range" min="1" max="8" step="1" value="4"></label>
  <span id="N-val">4</span>
`;
controls.style.marginTop = '12px';
controls.style.lineHeight = '1.8';
controls.style.color = '#eee';
document.getElementById('bats-real-plot').insertAdjacentElement('afterend', controls);

document.getElementById('B-slider').addEventListener('input', e => {
  Btoken = parseInt(e.target.value);
  document.getElementById('B-val').textContent = Btoken;
  drawPlot(Btoken, N);
});
document.getElementById('N-slider').addEventListener('input', e => {
  N = parseInt(e.target.value);
  document.getElementById('N-val').textContent = N;
  drawPlot(Btoken, N);
});
</script> <h3 id="predicting-a-trajectory">Predicting a trajectory</h3> <p>Once the LLM produces its <strong>action embedding</strong> $E_T^A$, a lightweight <strong>Action Model</strong> $\mathcal{A}_\theta$ (a three-layer MLP) transforms it into a trajectory $\tau_T$ - a sequence of predicted waypoints describing future motion.</p> \[E_T^A = \mathrm{LLM}(E_{1:T}^{1:N}, E_L), \\ \tau_T = \mathrm{ActionModel}(E_T^A).\] <p>To ensure stable behavior across embodiments (indoor navigation, UAVs, or cars), the predicted trajectory is <strong>normalized</strong> to the range $[-1, 1]$ and later <strong>rescaled</strong> by a task-specific factor $\alpha_{\text{task}}$. This prevents small prediction errors from exploding into large spatial deviations during waypoint rollout.</p> \[\tau_T = \{ \mathbf{a}_1, \ldots, \mathbf{a}_M \}_T = \alpha_{\text{task}} \cdot \mathcal{A}_\theta(E_T^A)\] <p>The navigation loss is computed as the <strong>mean squared error (MSE)</strong> between the predicted and ground-truth waypoints:</p> \[L_{\text{nav}} = \mathrm{MSE}(\tau_T^{\text{idx}}, \tau_{\text{gt}}^{\text{idx}})\] <p>Here, $\text{idx}$ represents the valid action dimensions:</p> <ul> <li><strong>Cars / wheeled robots:</strong> $\mathbf{a}^{\text{idx}} = (x, y, \theta)$</li> <li><strong>UAVs:</strong> $\mathbf{a}^{\text{idx}} = (x, y, z, \theta)$</li> </ul> <p>The model jointly learns navigation and question-answering by combining losses:</p> \[L = \beta L_{\text{nav}} + L_{\text{QA}}\] <p>where $\beta$ (set to 10) balances the navigation and QA terms. Because trajectory errors (MSE) are numerically small, $\beta$ amplifies their influence during optimization.</p> <div class="alert alert-info" role="alert"> 💡 Normalizing trajectory predictions before rescaling is crucial - it keeps the waypoint distribution consistent across tasks and prevents divergence in long-horizon navigation. </div> <hr/> <h2 id="learning-from-diverese-navigation-experiences">Learning from diverese navigation experiences</h2> <p>To train a foundation model capable of reasoning across embodiments and tasks, the author aggregate experience from multiple navigation datasets - spanning simulation, real-world driving, and language-conditioned control. Each dataset contributes a complementary view of embodied intelligence, from short-horizon indoor tasks to long-range, multi-agent driving.</p> <h3 id="navigation-data-composition">Navigation data Composition</h3> <p>The navigation data is collected across multiple datasets from different domains including, Vision-and-language navigation, object goal navigation, active visual tracking and autonomous driving.</p> <div class="table-responsive"> <table class="table table-bordered table-sm align-middle"> <thead> <tr> <th>Domain</th> <th>Action Space</th> <th>Trajectory Horizon (typ.)</th> <th>Key features</th> </tr> </thead> <tbody> <tr> <td>Indoor</td> <td>Discrete (F, L, R)</td> <td>Short (meters)</td> <td>Dense viewpoints, semantic goals</td> </tr> <tr> <td>UAV</td> <td>Continuous $(x,y,z,\theta)$</td> <td>Mid (10–30&nbsp;m)</td> <td>3D motion, altitude &amp; attitude control</td> </tr> <tr> <td>Driving</td> <td>Continuous $(x,y,\theta)$</td> <td>Long (20–80&nbsp;m)</td> <td>Multi-sensor context, traffic interaction</td> </tr> </tbody> <tfoot> <tr> <th colspan="4">Total navigation scale: <b>8.02&nbsp;M</b> trajectories</th> </tr> </tfoot> </table> </div> <h3 id="qa-datasets-composition">QA Datasets Composition</h3> <p>In addition to the navigation samples the model is trained to answer <strong>language QA corpora</strong>. The data include <strong>image QA</strong>, *<em>video QA</em></p> <div class="table-responsive"> <table class="table table-bordered table-sm align-middle"> <thead> <tr> <th>QA Family</th> <th>Count</th> <th>Input Modality</th> <th>Answer Space</th> <th>What it teaches</th> </tr> </thead> <tbody> <tr> <td><b>Image QA</b></td> <td><b>3.15&nbsp;M</b></td> <td>Image&nbsp;+&nbsp;Question</td> <td>Open-ended / MCQ</td> <td>Object &amp; attribute grounding, spatial relations, text-in-scene</td> </tr> <tr> <td><b>Video QA</b></td> <td><b>1.61&nbsp;M</b></td> <td>Video&nbsp;+&nbsp;Question</td> <td>Open-ended</td> <td>Temporal events, causality, activity understanding</td> </tr> </tbody> <tfoot> <tr> <th colspan="5">Total QA scale: <b>4.76&nbsp;M</b> Open-world understanding for perception &amp; reasoning</th> </tr> </tfoot> </table> </div> <div class="alert alert-info" role="alert"> 💡 Diversity in embodiment — from agents walking indoors to vehicles navigating cities — enables the model to generalize across vastly different action dynamics while sharing a unified model representation. </div> <hr/> <h2 id="training-paradigm">Training Paradigm</h2> <p>The model was trained on a 56×H100 GPU cluster for about 72 hours (≈4,000 GPU hours). Frames from QA datasets were sampled at 1 FPS to minimize redundancy, while discrete navigation data (e.g., Habitat) were sampled per action step, and continuous navigation tasks (e.g., EVT-Bench, nuScenes) at 2 FPS for efficiency.</p> <p>The visual encoders (DINOv2, SigLIP) and the language backbone (Qwen2-7B) were initialized from their pre-trained checkpoints. Following the standard VLM fine-tuning paradigm, only a subset of trainable parameters was fine-tuned for one epoch, ensuring efficient adaptation while preserving general multi-modal knowledge.</p> <hr/> <h2 id="deep-dive-into-main-results">Deep-dive into main results</h2> <p>The authors trying to answer the following questions:</p> <ul> <li>Performance on diverse cross-benchmark navigation tasks</li> <li>Performance on real-world environments</li> <li>Effectiveness of key design choices</li> </ul> <h3 id="visual-language-navigation-benchmarks">Visual language navigation benchmarks</h3> <p>VLN-CE vision-language navigation benchmark used to evaluate embodied agents that follow natural language instructions in realistic 3D environments. <strong>VLN</strong> visual-language-navigation and <strong>CE</strong> tends for continues 3d space environment.</p> <p>The VLN dataset is composed of R2R dataset [Anderson et al., 2018] and RxR [Ku et al., 2020] dataset. Both datasets provide natural-language route instructions paired with ground-truth trajectories.</p> <p><strong>NavFoM</strong> managed to achieve SOTA models in both single camera and multi-camera configuration without training on the specific camera configuration and with no additional inputs that are used in other methods (depth and odo).</p> <ul> <li><strong>VLN-CE RxR (Vision-Language Navigation)</strong> <ul> <li>Multi-camera: <strong>64.4% SR</strong> ↑ <em>(from 56.3%)</em></li> <li>Single-camera: <strong>57.4% SR</strong> ↑ <em>(from 51.8%)</em></li> </ul> </li> </ul> <p><strong>Success Rate (SR)</strong> measures how often the agent reaches the goal within a fixed distance threshold (commonly 3 m):</p> \[\text{SR} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}\!\left[d\big(p_T^{(i)},\, p_{\text{goal}}^{(i)}\big) &lt; \delta\right]\] <h3 id="openuav-banchmark">OpenUAV banchmark</h3> <p>NavFoM was also evaluated on TravelUAV (Wang et al., 2024), a challenging drone navigation benchmark where agents must follow natural-language instructions to reach outdoor targets over long-horizon flights (~200 m).</p> <p>Despite using only trajectories from the TravelUAV training split—without expert-collected supervision—NavFoM outperforms specialized UAV baselines, including TravelUAV itself, and does so <strong>without relying on downward-facing cameras</strong>.</p> <p>Performance drops notably on the Unseen-Map split, which involves ~300 m flights through unseen neighborhoods. This highlights the remaining challenge of generalizing to large-scale, unseen outdoor environments, where richer UAV data and exploration strategies are needed.</p> <div class="alert alert-info" role="alert"> 💡 Although NavFoM surpasses prior methods, its success rate remains far below human performance - achieving only 6–30% SR across UAV benchmark tasks </div> <h3 id="ovon-search-benchmark">OVON search benchmark</h3> <p>Object-Goal Vision-and-Language Navigation (<strong>OVON</strong>) is a challenging embodied navigation benchmarks, designed to test how well a single model can understand instructions, perceive scenes, and navigate to visual goals in 3D indoor environments.Unlike classical navigation tasks where the goal is a specific coordinate or room, OVON asks the model to find an object described in natural language - often open-vocabulary.</p> <p>So the model must:</p> <ul> <li> <p>Ground language → parse what object is being asked for.</p> </li> <li> <p>Perceive vision → detect potential object candidates.</p> </li> <li> <p>Act → plan a trajectory that leads to that object.</p> </li> </ul> <p>Similar to VLN-CE benchmark NavFoM manages to achieve SOTA results comparing to the best zero shot method and comparable results to best non-zero shot method without training or finetuning on the specific dataset configuration.</p> <ul> <li>Single-camera: <ul> <li>val-seen : <strong>37.7% SR</strong> ↓ <em>(from 55.0%)</em></li> <li>val-seen-synonym : <strong>43.3% SR</strong> ↓ <em>(from 45.0%)</em></li> <li>val-unseen : <strong>43.6% SR</strong> ↑ <em>(from 40.8%)</em></li> </ul> </li> <li>Multi-camera: <ul> <li>val-seen : <strong>40.1% SR</strong> ↓ <em>(from 55.0%)</em></li> <li>val-seen-synonym : <strong>45.4% SR</strong> ↑ <em>(from 45.0%)</em></li> <li>val-unseen : <strong>45.2% SR</strong> ↑ <em>(from 40.8%)</em></li> </ul> </li> </ul> <h3 id="tracking-evt-bench-benchmark">Tracking EVT-Bench benchmark</h3> <p>EVT-Bench (Embodied Visual Tracking Benchmark) is designed to evaluate an agent’s ability to visually track moving targets in embodied 3D environments. Unlike standard navigation tasks (where the goal is static), EVT-Bench requires continuous perception and motion control to follow a dynamic object—like another robot, human, or vehicle—through realistic indoor or outdoor scenes.</p> <p>It’s built within the Habitat simulation framework and includes both single-target and distracted-target settings:</p> <ul> <li><strong>Single Target</strong>: The agent follows one moving target throughout the episode.</li> <li><strong>Distracted Target</strong>: Multiple distractor objects appear; the agent must keep track of the correct one.</li> </ul> <p>NavFoM was evaluated on EVT-Bench for both Single Target and Distracted Target splits, under single-view and four-view camera setups. Trained only in the single-view setting, it still achieves state-of-the-art performance, outperforming TrackVLA (Wang et al., 2025), which was fine-tuned for tracking. When tested zero-shot with four cameras, performance improves slightly (+0.6% SR), though less than the gains seen in VLN tasks. This modest increase is attributed to EVT-Bench’s design, where most targets already appear in front of the robot, limiting the benefit of multi-view inputs.</p> <div class="alert alert-info" role="alert"> 💡 A follow-up work, TrackVLA++, published after NavFoM, reported substantial gains - improving success rate by over 12 points in the distracted-target setting compared to both the original TrackVLA and NavFoM. The authors attribute prior limitations to the lack of explicit spatial reasoning and robust temporal memory, which lead to failures under heavy occlusions or when distractors resemble the target. TrackVLA++ addresses these issues by introducing a Spatial Reasoning Module and a Target Identification Memory (TIM) for improved target continuity and discrimination. </div> <h3 id="autonomous-driving-benchmarks">Autonomous driving benchmarks</h3> <p>NavSim is a simulated benchmark for embodied autonomous driving, built to study how agents plan and control vehicles from raw perception and language or goal inputs. It features diverse, photorealistic driving environments with multiple viewpoints, simulating complex navigation tasks such as lane following, intersection turns, and obstacle avoidance. Evaluation typically involves six or eight-camera configurations, enabling 360° perception around the ego vehicle.</p> <p>NavFoM was evaluated on both NavSim and nuScenes under six and eight-view camera setups, without any configuration-specific fine-tuning. Despite not using explicit driving cues (like lane markings, traffic signals, or object tracking modules), NavFoM achieves performance comparable to state-of-the-art driving models on both benchmarks.</p> <p>The results highlight the model’s strong cross-embodiment generalization - a single navigation foundation model trained across diverse tasks can perform competitively even in autonomous-driving scenarios.</p> <p>The authors note that further gains could come from adding scene-level textual prompts (e.g., “follow the right lane and turn left at the intersection”), which may help integrate high-level reasoning with perception-driven control.</p> <div class="alert alert-info" role="alert"> 💡 Follow-up methods such as Hydra-Next (camera-only) and DriveDPO (camera + LiDAR) - both designed specifically for autonomous driving, achieve notably higher performance than NavFoM, with gains exceeding 4 points in PDMS. </div> <hr/> <h2 id="key-takeaways">Key takeaways</h2> <ul> <li> <p><strong>Unified navigation model</strong>: NavFoM integrates multiple robot embodiments and multiple navigation tasks into a single cross-modal foundation model.</p> </li> <li> <p><strong>Efficient temporal encoding</strong>: Introduces Temporal-Viewpoint Indicator (TVI) tokens to help the LLM understand when and from which view each token was captured.</p> </li> <li> <p><strong>Compute-aware memory</strong>: The Budget-Aware Temporal Sampling (BATS) strategy maintains long-term temporal reasoning under fixed token limits, improving real-world efficiency.</p> </li> <li> <p><strong>Strong generalization</strong>: Achieves competitive or state-of-the-art performance across VLN, OVON, EVT-Bench, and NavSim, without task-specific fine-tuning.</p> </li> <li> <p><strong>Early step forward</strong>: As the author state “NavFoM serves merely as a starting point toward a navigation foundation model”.</p> </li> <li> <p><strong>Future work</strong>: NavFoM can be integrated with some ideas on recent article (Hydra-Next, DriveDPO, etc) to improve on the regression based trajectory head.</p> </li> </ul> <h2 id="references">References</h2> <ul> <li>[Liu et al., 2025] <a href="https://arxiv.org/abs/2509.12129">Embodied Navigation Foundation Model</a></li> <li>[Oquab et al., 2024] <a href="https://arxiv.org/abs/2304.07193">DINOv2: Learning Robust Visual Features without Supervision</a></li> <li>[Zhai et al., 2023] <a href="https://arxiv.org/abs/2303.15343">SigLip</a></li> <li>[Wang et al., 2025] <a href="https://arxiv.org/abs/2505.23189">TrackVAL: Embodied Visual Tracking in the Wild</a></li> <li>[Liu et al., 2025] <a href="https://arxiv.org/abs/2510.07134">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a></li> <li>[Li et al., 2025] <a href="https://arxiv.org/abs/2503.12030">Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training</a></li> <li>[Shang et al., 2025] <a href="https://arxiv.org/abs/2509.17940">DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving</a></li> </ul> <div style="border-left:4px solid #999;padding:12px 16px;margin-top:2em;font-size:0.9em;"> 📚 <strong>Cite this blog </strong><br/> Portnoy, D. (2025). <em>Embodied-Drive.ai – Navigation meets foundation model.</em><br/> <a href="https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/">https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/</a> </div> <p style="margin-top:8px;"> 🔗 Share on <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/" target="_blank" rel="noopener noreferrer">LinkedIn</a> · <a href="https://twitter.com/intent/tweet?url=https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/&amp;text=Towards Universal Navigation Foundation Models" target="_blank" rel="noopener noreferrer">X</a> </p>]]></content><author><name>Doron Portnoy</name></author><category term="[&quot;embodied AI&quot;]"/><category term="navigation"/><category term="vision-language-action"/><category term="foundation models"/><summary type="html"><![CDATA[Exploring how a single Navigation Foundation Model generalizes across diverse robot embodiments and navigation tasks]]></summary></entry></feed>