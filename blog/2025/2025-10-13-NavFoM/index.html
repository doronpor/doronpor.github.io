<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Towards Universal Navigation Foundation Models | Doron Portnoy </title> <meta name="author" content="Doron Portnoy"> <meta name="description" content="Exploring how a single Navigation Foundation Model generalizes across diverse robot embodiments and navigation tasks"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/"> <script src="/assets/js/theme.js?bc4fb08bc6ab5de0ea96d0787b75e524"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Doron</span> Portnoy </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Towards Universal Navigation Foundation Models</h1> <p class="post-meta"> Created on October 13, 2025 by Doron Portnoy </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/navigation"> <i class="fa-solid fa-hashtag fa-sm"></i> navigation</a>   <a href="/blog/tag/vision-language-action"> <i class="fa-solid fa-hashtag fa-sm"></i> vision-language-action</a>   <a href="/blog/tag/foundation-models"> <i class="fa-solid fa-hashtag fa-sm"></i> foundation models</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper review</a>   ·   <a href="/blog/category/embodied-ai"> <i class="fa-solid fa-tag fa-sm"></i> embodied AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>This post reviews and summarizes the <em>Embodied Navigation Foundation Model</em> paper [Zhang et al., 2025], offering an independent overview and commentary on its approach to cross-embodiment and cross-task navigation.</p> </blockquote> <h2 id="navigation-meets-foundation-model">Navigation meets Foundation Model</h2> <p>Classical navigation models are often limited to a <strong>single robot or task</strong> - a ground robot model can’t generalize to a drone or a language-guided setup. Meanwhile, <strong>foundation models</strong> in vision and language (like CLIP and GPT-4V) have shown that large-scale multi-modal pretraining can yield broad generalization. In parallel, recent advancements in <strong>Vision-Language-Action (VLA)</strong> models have shown promise for <strong>end-to-end autonomous driving</strong>, where large language models (LLMs) are integrated with perception and planning modules to perform end-to-end reasoning and planning.</p> <p>Inspired by this, <a href="https://arxiv.org/abs/2509.12129" rel="external nofollow noopener" target="_blank">Embodied Navigation Foundation Model</a> introduced a cross-embodiment and cross-task navigation framework termed Navigation Foundation Model (<strong>NavFoM</strong>). NavFoM is trained on over eight million navigation samples spanning <strong>quadrupeds, drones, wheeled robots, and vehicles</strong>. The dataset covers a diverse set of tasks, including <strong>vision-and-language navigation</strong>, <strong>object searching</strong>, <strong>target tracking</strong>, and <strong>autonomous driving</strong> - allowing the model to generalize on cross-task and cross-embodiment.</p> <p>The experiments show that the <strong>NavFoM</strong> - makes significant progress toward generalist navigation. Without any task-specific fine-tuning, the model achieves <strong>state-of-the-art or competitive performance</strong> across multiple public task benchmarks and robot embodiments.</p> <figure align="center"> <img src="/assets/img/posts/ENFM_high_level_arch.jpg" width="90%"> <figcaption><b>Figure 1.</b> Architecture illustration. Adapted from <a href="https://arxiv.org/abs/2509.12129" rel="external nofollow noopener" target="_blank">Zhang et al., 2025</a>.</figcaption> </figure> <h3 id="key-results">Key results</h3> <ul> <li> <p><strong>VLN-CE RxR (Vision-Language Navigation)</strong></p> <ul> <li>Multi-camera: <strong>64.4% SR</strong> ↑ <em>(from 56.3%)</em> </li> <li>Single-camera: <strong>57.4% SR</strong> ↑ <em>(from 51.8%)</em> </li> </ul> </li> <li> <p><strong>HM3D-OVON (Object-Goal Navigation)</strong></p> <ul> <li>Zero-shot: <strong>45.2% SR</strong> ↑ (<em>from previous fine-tuned SOTA 43.%</em>)</li> </ul> </li> </ul> <hr> <h2 id="how-the-model-works">How the Model Works</h2> <p><strong>NavFoM</strong> employs a unified architecture that processes multi-modal navigation inputs from varying camera configurations and navigation horizons. The authors introduces two key innovations: <strong>Temporal-Viewpoint Indicator (TVI)</strong> tokens, which encode camera viewpoint and temporal context, and <strong>Budget-Aware Temporal Sampling (BATS)</strong>, which dynamically selects history tokens using a forgetting curve within a fixed token budget.</p> <h3 id="architecture-overview">Architecture Overview</h3> <p>The model consists of four major components:</p> <ol> <li> <p><strong>Vision Encoder</strong> - processes egocentric RGB from different camera configurations (single or multi-camera) based on <a href="https://arxiv.org/abs/2304.07193" rel="external nofollow noopener" target="_blank">DINOv2</a> and <a href="https://arxiv.org/abs/2303.15343" rel="external nofollow noopener" target="_blank">SigLip</a>.</p> </li> <li> <p><strong>Cross-Modal projector</strong> - Vision encoder is followed by a cross-modal projector to align the image embeddings to the text embeddings.</p> </li> <li> <p><strong>Large Language Model</strong> - receives text instructions, projected image embeddings and temporal viewpoint indicator tokens and output tokens to the Action head. The model uses a pretrained large language model (LLM) backbone <a href="https://arxiv.org/abs/2407.10671" rel="external nofollow noopener" target="_blank">Qwen2-7B</a>.</p> </li> <li> <p><strong>Trajectory prediction head</strong> - outputs continuous trajectory, normalized to [-1,1] and scaled according to the output task. Trajectory head is Implemented as a three-layer MLP.</p> </li> </ol> <figure align="center"> <img src="/assets/img/posts/navform_pipeline.jpg" width="90%"> <figcaption><b>Figure 2.</b> NavForm pipeline. Adapted from <a href="https://arxiv.org/abs/2509.12129" target="_blank" rel="external nofollow noopener">Zhang et al., 2025</a>.</figcaption> </figure> <h3 id="encoding-temporal-information-and-camera-setup">Encoding temporal information and camera setup</h3> <blockquote> <p><strong>Challenge</strong>: model must understand when and where (which view) each observation was captured.</p> </blockquote> <p>A key challenge in multi-view navigation is that visual tokens alone don’t tell the model which camera or timestep they came from. To address this, NavFoM introduces <strong>Temporal-Viewpoint Indicator (TVI)</strong> tokens — lightweight learnable embeddings that tag each frame with both its viewpoint angle and its temporal position in the sequence.</p> <p>Each TVI token combines three components:</p> <ul> <li> <p><strong>Angle embedding</strong>: captures the camera’s azimuth in a circular way (0° ≡ 360°) so nearby viewpoints are geometrically close in embedding space.</p> </li> <li> <p><strong>Time embedding</strong>: marks the temporal order of frames, allowing reasoning over navigation history.</p> </li> <li> <p><strong>Base embedding</strong>: provides a consistent token structure that the model can adapt across different tasks (navigation, video QA, image QA).</p> </li> </ul> \[\mathbf{E}_{\text{TVIT}} = \begin{cases} \mathbf{E}_{\text{Base}} + \mathcal{P}_{\text{time}}\big(\text{TimePE}(t)\big) + \mathcal{P}_{\text{angle}}\big(\text{AnglePE}(\phi)\big), &amp; \text{if Navigation} \\ \mathbf{E}_{\text{Base}} + \mathcal{P}_{\text{time}}\big(\text{TimePE}(t)\big), &amp; \text{if Video QA} \\ \mathbf{E}_{\text{Base}}, &amp; \text{if Image QA} \end{cases}\] <p>This flexible design lets NavFoM process arbitrary camera configurations from single-view robots to panoramic car rigs, while maintaining clear separation between views and timestep.</p> <div class="alert alert-info" role="alert"> 💡 This method only incorporate a single DOF to encode the view of the camera. This may be restrictive in some situations like UAV with downward facing camera (see evaluation section) </div> <h3 id="adapting-to-token-budget">Adapting to token budget</h3> <blockquote> <p><strong>Challenge:</strong> Balancing long-term reasoning with limited compute and speed in real world</p> </blockquote> <p>Long navigation sequences can produce thousands of visual tokens — far more than a transformer can handle efficiently for real time real world scenarios. To maintain temporal reasoning without exceeding memory limits, <strong>NavFoM</strong> introduces <strong>Budget-Aware Temporal Sampling (BATS)</strong>.</p> <p>Rather than treating every timestep equally, <strong>BATS applies a forgetting curve</strong> that assigns higher priority to recent frames and lower priority to distant ones. From all past frames {1, 2, …, T}, the model selects a subset B:</p> \[|\mathcal{S}(T)| = B, \quad \mathcal{S}(T) \subseteq \{1, 2, \ldots, T\}\] <p>The forgetting curve is modeled as a simple exponential-like decay:</p> \[P(t, T) \propto e^{-\lambda (T - t)}\] <p>This ensures the model maintains a <strong>constant token budget</strong> while focusing attention on the most recent and relevant observations. The result is a <strong>balanced temporal representation</strong> that achieves near-equivalent performance to full-sequence training while significantly reducing inference time.</p> <p>Explore the sampling function:</p> <div class="alert alert-info" role="alert"> 💡 Adjusting to high budget with low view count will sample each frame - Try it !. </div> \[P(t) = (1 - \varepsilon)e^{k(t - T)/T} + \varepsilon, \quad E_{\text{frames}} = T \left[ (1 - \varepsilon)\frac{1 - e^{-k}}{k} + \varepsilon \right]\] <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <div id="bats-real-plot" style="width:100%;height:420px;"></div> <script>
function E_frames(k, eps, T) {
  // Stable version for small k
  if (Math.abs(k) < 1e-6) return T;
  return T * ((1 - eps) * (1 - Math.exp(-k)) / k + eps);
}

function solveK(target_E, eps, T) {
  // adaptive bounds
  let low = 1e-6, high = 50;
  const target = target_E;
  const fLow = E_frames(low, eps, T);
  const fHigh = E_frames(high, eps, T);

  // if outside range, clamp
  if (target >= fLow) return low;
  if (target <= fHigh) return high;

  for (let i = 0; i < 80; i++) {
    const mid = 0.5 * (low + high);
    const fMid = E_frames(mid, eps, T);
    if (fMid > target) low = mid;
    else high = mid;
  }
  return 0.5 * (low + high);
}

function generateData(Btoken=1600, N=4, eps=0.1, T=125) {
  let E_target = (Btoken / N - 65) / 5;
  if (!isFinite(E_target) || E_target < 1) E_target = 1;
  if (E_target > T - 1) E_target = T - 1;

  const k = solveK(E_target, eps, T);
  const t = Array.from({length: T}, (_, i) => i + 1);
  const P = t.map(x => (1 - eps) * Math.exp(k * (x - T) / T) + eps);
  const maxP = Math.max(...P);
  const normP = P.map(p => p / maxP);
  return {t, P: normP, k, E_target};
}

function drawPlot(Btoken, N) {
  const eps = 0.1, T = 125;
  const {t, P, k, E_target} = generateData(Btoken, N, eps, T);
  const trace = {
    x: t,
    y: P,
    type: 'scatter',
    mode: 'lines',
    line: {color: '#00bcd4', width: 4},
    fill: 'tozeroy',
    fillcolor: 'rgba(0,188,212,0.25)',
  };
  const layout = {
    margin: {t: 50, l: 60, r: 20, b: 50},
    title: {text: `BATS — B tokens=${Btoken}, N=${N}, k≈${k.toFixed(2)}, E frames ≈${E_target.toFixed(1)}`, font: {size: 14}},
    xaxis: {title: 'timestep (t)', gridcolor: 'rgba(255,255,255,0.1)'},
    yaxis: {title: 'P(t)', gridcolor: 'rgba(255,255,255,0.1)'},
    plot_bgcolor: 'rgba(0,0,0,0)',
    paper_bgcolor: 'rgba(0,0,0,0)',
    font: {color: '#eee', size: 14}
  };
  Plotly.newPlot('bats-real-plot', [trace], layout, {displayModeBar: false});
}

let Btoken = 1600, N = 4;
drawPlot(Btoken, N);

const controls = document.createElement('div');
controls.innerHTML = `
  <label>B tokens: <input id="B-slider" type="range" min="640" max="2048" step="64" value="1600"></label>
  <span id="B-val">1600</span><br>
  <label>N views: <input id="N-slider" type="range" min="1" max="8" step="1" value="4"></label>
  <span id="N-val">4</span>
`;
controls.style.marginTop = '12px';
controls.style.lineHeight = '1.8';
controls.style.color = '#eee';
document.getElementById('bats-real-plot').insertAdjacentElement('afterend', controls);

document.getElementById('B-slider').addEventListener('input', e => {
  Btoken = parseInt(e.target.value);
  document.getElementById('B-val').textContent = Btoken;
  drawPlot(Btoken, N);
});
document.getElementById('N-slider').addEventListener('input', e => {
  N = parseInt(e.target.value);
  document.getElementById('N-val').textContent = N;
  drawPlot(Btoken, N);
});
</script> <h3 id="predicting-a-trajectory">Predicting a trajectory</h3> <p>Once the LLM produces its <strong>action embedding</strong> $E_T^A$, a lightweight <strong>Action Model</strong> $\mathcal{A}_\theta$ (a three-layer MLP) transforms it into a trajectory $\tau_T$ - a sequence of predicted waypoints describing future motion.</p> \[E_T^A = \mathrm{LLM}(E_{1:T}^{1:N}, E_L), \\ \tau_T = \mathrm{ActionModel}(E_T^A).\] <p>To ensure stable behavior across embodiments (indoor navigation, UAVs, or cars), the predicted trajectory is <strong>normalized</strong> to the range $[-1, 1]$ and later <strong>rescaled</strong> by a task-specific factor $\alpha_{\text{task}}$. This prevents small prediction errors from exploding into large spatial deviations during waypoint rollout.</p> \[\tau_T = \{ \mathbf{a}_1, \ldots, \mathbf{a}_M \}_T = \alpha_{\text{task}} \cdot \mathcal{A}_\theta(E_T^A)\] <p>The navigation loss is computed as the <strong>mean squared error (MSE)</strong> between the predicted and ground-truth waypoints:</p> \[L_{\text{nav}} = \mathrm{MSE}(\tau_T^{\text{idx}}, \tau_{\text{gt}}^{\text{idx}})\] <p>Here, $\text{idx}$ represents the valid action dimensions:</p> <ul> <li> <strong>Cars / wheeled robots:</strong> $\mathbf{a}^{\text{idx}} = (x, y, \theta)$</li> <li> <strong>UAVs:</strong> $\mathbf{a}^{\text{idx}} = (x, y, z, \theta)$</li> </ul> <p>The model jointly learns navigation and question-answering by combining losses:</p> \[L = \beta L_{\text{nav}} + L_{\text{QA}}\] <p>where $\beta$ (set to 10) balances the navigation and QA terms. Because trajectory errors (MSE) are numerically small, $\beta$ amplifies their influence during optimization.</p> <div class="alert alert-info" role="alert"> 💡 In planning-oriented tasks such as the NavSim benchmark, recent works (e.g., the Hydra series) have replaced the single-output regression head with multi-modal planning formulations, reflecting the view that planning inherently involves multiple possible futures. </div> <hr> <h2 id="learning-from-diverse-navigation-experiences">Learning from diverse navigation experiences</h2> <p>To train a foundation model capable of reasoning across embodiments and tasks, the author aggregate experience from multiple navigation datasets - spanning simulation, real-world driving, and language-conditioned control. Each dataset contributes a complementary view of embodied intelligence, from short-horizon indoor tasks to long-range, multi-agent driving.</p> <h3 id="navigation-data-composition">Navigation data Composition</h3> <p>The navigation data is collected across multiple datasets from different domains including, Vision-and-language navigation, object goal navigation, active visual tracking and autonomous driving.</p> <div class="table-responsive"> <table class="table table-bordered table-sm align-middle"> <thead> <tr> <th>Domain</th> <th>Action Space</th> <th>Trajectory Horizon (typ.)</th> <th>Key features</th> </tr> </thead> <tbody> <tr> <td>Indoor</td> <td>Discrete (F, L, R)</td> <td>Short (meters)</td> <td>Dense viewpoints, semantic goals</td> </tr> <tr> <td>UAV</td> <td>Continuous $(x,y,z,\theta)$</td> <td>Mid (10–30 m)</td> <td>3D motion, altitude &amp; attitude control</td> </tr> <tr> <td>Driving</td> <td>Continuous $(x,y,\theta)$</td> <td>Long (20–80 m)</td> <td>Multi-sensor context, traffic interaction</td> </tr> </tbody> <tfoot> <tr> <th colspan="4">Total navigation scale: <b>8.02 M</b> trajectories</th> </tr> </tfoot> </table> </div> <h3 id="qa-datasets-composition">QA Datasets Composition</h3> <p>In addition to the navigation samples the model is trained to answer <strong>language QA corpora</strong>. The data include <strong>image QA</strong>, <strong>video QA</strong></p> <div class="table-responsive"> <table class="table table-bordered table-sm align-middle"> <thead> <tr> <th>QA Family</th> <th>Count</th> <th>Input Modality</th> <th>Answer Space</th> <th>What it teaches</th> </tr> </thead> <tbody> <tr> <td><b>Image QA</b></td> <td><b>3.15 M</b></td> <td>Image + Question</td> <td>Open-ended / MCQ</td> <td>Object &amp; attribute grounding, spatial relations, text-in-scene</td> </tr> <tr> <td><b>Video QA</b></td> <td><b>1.61 M</b></td> <td>Video + Question</td> <td>Open-ended</td> <td>Temporal events, causality, activity understanding</td> </tr> </tbody> <tfoot> <tr> <th colspan="5">Total QA scale: <b>4.76 M</b> Open-world understanding for perception &amp; reasoning</th> </tr> </tfoot> </table> </div> <div class="alert alert-info" role="alert"> 💡 Diversity in embodiment — from agents walking indoors to vehicles navigating cities — enables the model to generalize across vastly different action dynamics while sharing a unified model representation. </div> <hr> <h2 id="training-paradigm">Training Paradigm</h2> <p>The model was trained on a 56×H100 GPU cluster for about 72 hours (≈4,000 GPU hours). Frames from QA datasets were sampled at 1 FPS to minimize redundancy, while discrete navigation data (e.g., Habitat) were sampled per action step, and continuous navigation tasks (e.g., EVT-Bench, nuScenes) at 2 FPS for efficiency.</p> <p>The visual encoders (DINOv2, SigLIP) and the language backbone (Qwen2-7B) were initialized from their pre-trained checkpoints. Following the standard VLM fine-tuning paradigm, only a subset of trainable parameters was fine-tuned for one epoch, ensuring efficient adaptation while preserving general multi-modal knowledge.</p> <hr> <h2 id="deep-dive-into-main-results">Deep-dive into main results</h2> <p>The authors trying to answer the following questions:</p> <ul> <li>Performance on diverse cross-benchmark navigation tasks</li> <li>Performance on real-world environments</li> <li>Effectiveness of key design choices</li> </ul> <h3 id="visual-language-navigation-benchmarks">Visual language navigation benchmarks</h3> <p>VLN-CE vision-language navigation benchmark used to evaluate embodied agents that follow natural language instructions in realistic 3D environments. <strong>VLN</strong> visual-language-navigation and <strong>CE</strong> stands for continues 3d space environment.</p> <p>The VLN dataset is composed of R2R dataset [Anderson et al., 2018] and RxR [Ku et al., 2020] dataset. Both datasets provide natural-language route instructions paired with ground-truth trajectories.</p> <p><strong>NavFoM</strong> managed to achieve SOTA results in both single camera and multi-camera configuration without training on the specific camera configuration and with no additional inputs that are used in other methods (depth and odo).</p> <ul> <li> <strong>VLN-CE RxR (Vision-Language Navigation)</strong> <ul> <li>Multi-camera: <strong>64.4% SR</strong> ↑ <em>(from 56.3%)</em> </li> <li>Single-camera: <strong>57.4% SR</strong> ↑ <em>(from 51.8%)</em> </li> </ul> </li> </ul> <p><strong>Success Rate (SR)</strong> measures how often the agent reaches the goal within a fixed distance threshold (commonly 3 m):</p> \[\text{SR} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}\!\left[d\big(p_T^{(i)},\, p_{\text{goal}}^{(i)}\big) &lt; \delta\right]\] <h3 id="openuav-benchmark">OpenUAV benchmark</h3> <p>NavFoM was also evaluated on TravelUAV (Wang et al., 2024), a challenging drone navigation benchmark where agents must follow natural-language instructions to reach outdoor targets over long-horizon flights (~200 m).</p> <p>Despite using only trajectories from the TravelUAV training split, without expert-collected supervision, NavFoM outperforms specialized UAV baselines, including TravelUAV itself, and does so <strong>without relying on downward-facing cameras</strong>.</p> <p>Performance drops notably on the Unseen-Map split, which involves ~300 m flights through unseen neighborhoods. This highlights the remaining challenge of generalizing to large-scale, unseen outdoor environments, where richer UAV data and exploration strategies are needed.</p> <div class="alert alert-info" role="alert"> 💡 Although NavFoM surpasses prior methods, its success rate remains far below human performance - achieving only 6–30% SR across UAV benchmark tasks </div> <h3 id="ovon-search-benchmark">OVON search benchmark</h3> <p>Object-Goal Vision-and-Language Navigation (<strong>OVON</strong>) is a challenging embodied navigation benchmarks, designed to test how well a single model can understand instructions, perceive scenes, and navigate to visual goals in 3D indoor environments.Unlike classical navigation tasks where the goal is a specific coordinate or room, OVON asks the model to find an object described in natural language - often open-vocabulary.</p> <p>So the model must:</p> <ul> <li> <p>Ground language → parse what object is being asked for.</p> </li> <li> <p>Perceive vision → detect potential object candidates.</p> </li> <li> <p>Act → plan a trajectory that leads to that object.</p> </li> </ul> <p>Similar to VLN-CE benchmark NavFoM manages to achieve SOTA results comparing to the best zero shot method and comparable results to best non-zero shot method without training or finetuning on the specific dataset configuration.</p> <ul> <li>Single-camera: <ul> <li>val-seen : <strong>37.7% SR</strong> ↓ <em>(from 55.0%)</em> </li> <li>val-seen-synonym : <strong>43.3% SR</strong> ↓ <em>(from 45.0%)</em> </li> <li>val-unseen : <strong>43.6% SR</strong> ↑ <em>(from 40.8%)</em> </li> </ul> </li> <li>Multi-camera: <ul> <li>val-seen : <strong>40.1% SR</strong> ↓ <em>(from 55.0%)</em> </li> <li>val-seen-synonym : <strong>45.4% SR</strong> ↑ <em>(from 45.0%)</em> </li> <li>val-unseen : <strong>45.2% SR</strong> ↑ <em>(from 40.8%)</em> </li> </ul> </li> </ul> <h3 id="tracking-evt-bench-benchmark">Tracking EVT-Bench benchmark</h3> <p>EVT-Bench (Embodied Visual Tracking Benchmark) is designed to evaluate an agent’s ability to visually track moving targets in embodied 3D environments. Unlike standard navigation tasks (where the goal is static), EVT-Bench requires continuous perception and motion control to follow a dynamic object, like another robot, human, or vehicle through realistic indoor or outdoor scenes.</p> <p>It’s built within the Habitat simulation framework and includes both single-target and distracted-target settings:</p> <ul> <li> <strong>Single Target</strong>: The agent follows one moving target throughout the episode.</li> <li> <strong>Distracted Target</strong>: Multiple distractor objects appear; the agent must keep track of the correct one.</li> </ul> <p>NavFoM was evaluated on EVT-Bench for both single target and distracted target splits, under single-view and four-view camera setups. Trained only in the single-view setting, it still achieves state-of-the-art performance, outperforming TrackVLA (Wang et al., 2025), which was fine-tuned for tracking. When tested zero-shot with four cameras, performance improves slightly (+0.6% SR), though less than the gains seen in VLN tasks. This modest increase is attributed to EVT-Bench’s design, where most targets already appear in front of the robot, limiting the benefit of multi-view inputs.</p> <div class="alert alert-info" role="alert"> 💡 A follow-up work, TrackVLA++, published after NavFoM, reported substantial gains - improving success rate by over 12 points in the distracted-target setting compared to both the original TrackVLA and NavFoM. The authors attribute prior limitations to the lack of explicit spatial reasoning and robust temporal memory, which lead to failures under heavy occlusions or when distractors resemble the target. TrackVLA++ addresses these issues by introducing a Spatial Reasoning Module and a Target Identification Memory (TIM) for improved target continuity and discrimination. </div> <h3 id="autonomous-driving-benchmarks">Autonomous driving benchmarks</h3> <p>NavSim is a simulated benchmark for embodied autonomous driving, built to study how agents plan and control vehicles from raw perception and language or goal inputs. It features diverse, photorealistic driving environments with multiple viewpoints, simulating complex navigation tasks such as lane following, intersection turns, and obstacle avoidance. Evaluation typically involves six or eight-camera configurations, enabling 360° perception around the ego vehicle.</p> <p>NavFoM was evaluated on both NavSim and nuScenes under six and eight-view camera setups, without any configuration-specific fine-tuning. Despite not using explicit driving cues (like lane markings, traffic signals, or object tracking modules), NavFoM achieves performance comparable to state-of-the-art driving models on both benchmarks.</p> <p>The results highlight the model’s strong cross-embodiment generalization - a single navigation foundation model trained across diverse tasks can perform competitively even in autonomous-driving scenarios.</p> <p>The authors note that further gains could come from adding scene-level textual prompts (e.g., “follow the right lane and turn left at the intersection”), which may help integrate high-level reasoning with perception-driven control.</p> <div class="alert alert-info" role="alert"> 💡 Follow-up methods such as Hydra-Next (camera-only) and DriveDPO (camera + LiDAR) - both designed specifically for autonomous driving, achieve notably higher performance than NavFoM, with gains exceeding 4 points in PDMS. </div> <hr> <h2 id="key-takeaways">Key takeaways</h2> <ul> <li> <p><strong>Unified navigation model</strong>: NavFoM integrates multiple robot embodiments and multiple navigation tasks into a single cross-modal foundation model.</p> </li> <li> <p><strong>Efficient temporal encoding</strong>: Introduces Temporal-Viewpoint Indicator (TVI) tokens to help the LLM understand when and from which view each token was captured.</p> </li> <li> <p><strong>Compute-aware memory</strong>: The Budget-Aware Temporal Sampling (BATS) strategy maintains long-term temporal reasoning under fixed token limits, improving real-world efficiency.</p> </li> <li> <p><strong>Strong generalization</strong>: Achieves competitive or state-of-the-art performance across VLN, OVON, EVT-Bench, and NavSim, without task-specific fine-tuning.</p> </li> <li> <p><strong>Early step forward</strong>: As the author state “NavFoM serves merely as a starting point toward a navigation foundation model”.</p> </li> <li> <p><strong>Future work</strong>: NavFoM can be integrated with some ideas on recent article (Hydra-Next, DriveDPO, etc) to improve on the regression based trajectory head.</p> </li> </ul> <h2 id="references">References</h2> <ul> <li>[Zhang et al., 2025] <a href="https://arxiv.org/abs/2509.12129" rel="external nofollow noopener" target="_blank">Embodied Navigation Foundation Model</a> </li> <li>[Oquab et al., 2024] <a href="https://arxiv.org/abs/2304.07193" rel="external nofollow noopener" target="_blank">DINOv2: Learning Robust Visual Features without Supervision</a> </li> <li>[Zhai et al., 2023] <a href="https://arxiv.org/abs/2303.15343" rel="external nofollow noopener" target="_blank">SigLip</a> </li> <li>[Wang et al., 2025] <a href="https://arxiv.org/abs/2505.23189" rel="external nofollow noopener" target="_blank">TrackVAL: Embodied Visual Tracking in the Wild</a> </li> <li>[Liu et al., 2025] <a href="https://arxiv.org/abs/2510.07134" rel="external nofollow noopener" target="_blank">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a> </li> <li>[Li et al., 2025] <a href="https://arxiv.org/abs/2503.12030" rel="external nofollow noopener" target="_blank">Hydra-NeXt: Robust Closed-Loop Driving with Open-Loop Training</a> </li> <li>[Shang et al., 2025] <a href="https://arxiv.org/abs/2509.17940" rel="external nofollow noopener" target="_blank">DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving</a> </li> </ul> <div style="border-left:4px solid #999;padding:12px 16px;margin-top:2em;font-size:0.9em;"> 📚 <strong>Cite this blog </strong><br> Portnoy, D. (2025). <em>Embodied-Drive.ai – Navigation meets foundation model.</em><br> <a href="https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/">https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/</a> </div> <p style="margin-top:8px;"> 🔗 Share on <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/" target="_blank" rel="noopener noreferrer">LinkedIn</a> · <a href="https://twitter.com/intent/tweet?url=https://doronpor.github.io/blog/2025/2025-10-13-NavFoM/&amp;text=Towards%20Universal%20Navigation%20Foundation%20Models" target="_blank" rel="noopener noreferrer">X</a> </p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script defer src="/assets/js/giscus-setup.js"></script> <noscript> Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Doron Portnoy. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>